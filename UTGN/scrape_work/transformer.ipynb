{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T02:22:17.743350Z",
     "start_time": "2019-06-26T02:22:14.789934Z"
    }
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Debugging nans in the backward pass](https://stackoverflow.com/questions/34046048/debugging-nans-in-the-backward-pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:49:51.453072Z",
     "start_time": "2019-06-26T08:49:51.384176Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Encoder portion of the transformer.\n",
    "\n",
    "Input tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES]\n",
    "Output tensor of the same shape.\n",
    "\n",
    "# TODO: Take a look at _higher_recurrence and perform similar \n",
    "functions(like updating config, etc)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "cast32 = lambda x: tf.dtypes.cast(x, tf.float32)\n",
    "\n",
    "def _get_mean_std(x):\n",
    "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    squared = tf.square(x - mean)\n",
    "    variance = tf.reduce_mean(squared, axis=-1, keepdims=True)\n",
    "    std = tf.sqrt(variance)\n",
    "    return mean, std\n",
    "\n",
    "def _layer_norm(layer):\n",
    "    with tf.variable_scope(\"norm\"):\n",
    "        scale = tf.get_variable(\"scale\", shape=layer.shape[-1], dtype=tf.float32)\n",
    "        base = tf.get_variable(\"base\", shape=layer.shape[-1], dtype=tf.float32)\n",
    "        mean, std = _get_mean_std(layer)\n",
    "        norm = (layer - mean) / (std + 1e-6)\n",
    "        return norm * scale + base\n",
    "    \n",
    "    \n",
    "def _attention(query, key, value, mask, keep_prob):\n",
    "    \"\"\"Calculates scaled dot-product attention.\n",
    "    \n",
    "    softmax(Q K^{T} / sqrt(d_{k}))V\n",
    "    \n",
    "    Args:\n",
    "        query: A query tensor of shape [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES].\n",
    "        key:  The key tensor.\n",
    "        value: The value tensor.\n",
    "        mask: Mask of shape [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES]\n",
    "        keep_prob: The drop out probability.\n",
    "    \n",
    "    Returns:\n",
    "        The scaled dot-product attention.\n",
    "        Shape: [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES]\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.shape[-1].value\n",
    "    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2]))\n",
    "    scores = scores / tf.constant(np.sqrt(d_k), dtype=tf.float32)\n",
    "    mask_add = ((scores * 0) - cast32(1e9)) * (tf.constant(1.) - cast32(mask))\n",
    "    scores = scores * cast32(mask) + mask_add\n",
    "    attn = tf.nn.softmax(scores, axis=-1)\n",
    "    attn = tf.nn.dropout(attn, keep_prob)\n",
    "    return tf.matmul(attn, value)\n",
    "\n",
    "\n",
    "def _prepare_multi_head_attention(x, heads, name):\n",
    "    \"\"\"Prepares for multihead attention.\n",
    "    \n",
    "    Prepares query, key, value that have form [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor input.\n",
    "        heads: Number of heads.\n",
    "        name: Either query, key, or value.\n",
    "    \n",
    "    Returns:\n",
    "        A prepared Q, K, or V of form [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES]\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: Dimension of features must be divisible by the number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    n_batches, seq_len, d_model = x.shape\n",
    "    assert d_model % heads == 0, \"Dimension of features needs to be divisible by the number of heads.\"\n",
    "    d_k = d_model // heads\n",
    "    x = tf.layers.dense(x, units=d_model, name=name)\n",
    "    x = tf.reshape(x, shape=[n_batches, seq_len, heads, d_k])\n",
    "    x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _multi_head_attention(query, key, value, mask, heads, keep_prob):\n",
    "    \"\"\"Calculates the multihead attention.\n",
    "    \n",
    "    Args:\n",
    "        query: query tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        key: key tensor.\n",
    "        value: value tensor.\n",
    "        mask: mask tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        heads: number of heads.\n",
    "        keep_prob: The drop out probability.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"multi_head\"):\n",
    "        n_batches, seq_len, d_model = query.shape\n",
    "        query = _prepare_multi_head_attention(query, heads, \"query\")\n",
    "        key = _prepare_multi_head_attention(key, heads, \"key\")\n",
    "        value = _prepare_multi_head_attention(value, heads, \"value\")\n",
    "        mask = tf.expand_dims(mask, axis=1)\n",
    "        out = _attention(query, key, value, mask=mask, keep_prob=keep_prob)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        out = tf.reshape(out, shape=[n_batches, seq_len, d_model])\n",
    "        return tf.layers.dense(out, units=d_model, name=\"attention\")\n",
    "\n",
    "\n",
    "def _feed_forward(x, d_model, d_ff, keep_prob):\n",
    "    \"\"\"Feed forward layer along with of relu and dropout.\n",
    "    \n",
    "    FFN(x) = max(0,xW1+b1)W2+b2\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor.\n",
    "        d_model: dimension of W2.\n",
    "        d_ff: dimension of W1.\n",
    "        keep_prob: The drop out probability.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"feed_forward\"):\n",
    "        hidden = tf.layers.dense(x, units=d_ff, name=\"hidden\")\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        hidden = tf.nn.dropout(hidden, keep_prob=keep_prob)\n",
    "        return tf.layers.dense(hidden, units=d_model, name=\"out\")\n",
    "\n",
    "\n",
    "def _encoder_layer(x, mask, layer_num, heads, keep_prob, d_ff):\n",
    "    \"\"\"Create a single encoder layer.\n",
    "    \n",
    "    Args:\n",
    "        x: input tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        mask: mask tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        layer_num: The number label of an encoder layer.\n",
    "        heads: Number of heads.\n",
    "        keep_prob: The drop out probability.\n",
    "        d_ff: dimension of W1.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \"\"\"\n",
    "\n",
    "    d_model = x.shape[-1]\n",
    "    # with tf.variable_scope(f\"attention_{layer_num}\"):\n",
    "    with tf.variable_scope(\"attention_\" + str(layer_num)):\n",
    "        attention_out = _multi_head_attention(x,\n",
    "                                              x,\n",
    "                                              x,\n",
    "                                              mask=mask,\n",
    "                                              heads=heads,\n",
    "                                              keep_prob=keep_prob)\n",
    "        added = x + tf.nn.dropout(attention_out, keep_prob)\n",
    "        #         x = tf.contrib.layers.layer_norm(added)\n",
    "        x = _layer_norm(added)\n",
    "        # x = added\n",
    "\n",
    "    # with tf.variable_scope(f\"ff_{layer_num}\"):\n",
    "    with tf.variable_scope(\"ff_\" + str(layer_num)):\n",
    "        ff_out = _feed_forward(x, d_model, d_ff, keep_prob)\n",
    "        added = x + tf.nn.dropout(ff_out, keep_prob)\n",
    "        #         return tf.contrib.layers.layer_norm(added)\n",
    "        return _layer_norm(added)\n",
    "        # return added\n",
    "\n",
    "\n",
    "def _encoder(x, mask, n_layers, heads, keep_prob, d_ff):\n",
    "    \"\"\"Create the encoder architecture\n",
    "    \n",
    "    Args:\n",
    "        x: input tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        mask: mask tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        n_layers: number of layers of the encoder model.\n",
    "        heads: number of heads.\n",
    "        keep_prob: The drop out probability.\n",
    "        d_ff: dimension of W1.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        for i in range(n_layers):\n",
    "            x = _encoder_layer(x,\n",
    "                               mask=mask,\n",
    "                               layer_num=i,\n",
    "                               heads=heads,\n",
    "                               keep_prob=keep_prob,\n",
    "                               d_ff=d_ff)\n",
    "        return x\n",
    "\n",
    "\n",
    "# def _generate_positional_encodings(d_model, seq_len=5000, max_len=5000):\n",
    "#     \"\"\"Create positional encoding.\n",
    "    \n",
    "#     Args:\n",
    "#         d_model: dimension of input embeddings\n",
    "#         seq_len: maximum sequence length of batch\n",
    "        \n",
    "#     Returns:\n",
    "#         Constant tensor of shape [1, seq_len, d_model]\n",
    "#     \"\"\"\n",
    "\n",
    "#     encodings = tf.zeros((seq_len, d_model), dtype=float)\n",
    "#     position = tf.range(0, seq_len)\n",
    "#     position = tf.reshape(position, (seq_len, 1))\n",
    "#     position = cast32(position)\n",
    "#     two_i = tf.range(0, d_model, 2)\n",
    "    \n",
    "#     div_term = -tf.math.log(10000.0) * cast32(two_i)\n",
    "#     div_term = div_term / tf.dtypes.cast(d_model, tf.float32)\n",
    "#     div_term = tf.math.exp(div_term)\n",
    "    \n",
    "# #     div_term = tf.math.exp(-tf.math.log(10000.0) * two_i / tf.dtypes.cast(d_model, tf.float32))\n",
    "#     encodings[:, 0::2] = tf.math.sin(position * div_term)\n",
    "#     encodings[:, 1::2] = tf.math.cos(position * div_term)\n",
    "    \n",
    "#     return tf.constant(encodings.reshape((1, seq_len, d_model)),\n",
    "#                        dtype=tf.float32,\n",
    "#                        name=\"positional_encodings\")\n",
    "\n",
    "\n",
    "\n",
    "def _generate_positional_encodings(d_model, seq_len=5000):\n",
    "    \"\"\"Create positional encoding.\n",
    "    \n",
    "    Args:\n",
    "        d_model: dimension of input embeddings\n",
    "        seq_len: maximum sequence length of batch\n",
    "        \n",
    "    Returns:\n",
    "        Constant tensor of shape [1, seq_len, d_model]\n",
    "    \"\"\"\n",
    "\n",
    "    encodings = np.zeros((seq_len, d_model), dtype=float)\n",
    "    position = np.arange(0, seq_len).reshape((seq_len, 1))\n",
    "    two_i = np.arange(0, d_model, 2)\n",
    "    div_term = np.exp(-np.log(10000.0) * two_i / d_model)\n",
    "    encodings[:, 0::2] = np.sin(position * div_term)\n",
    "    encodings[:, 1::2] = np.cos(position * div_term)\n",
    "    return tf.constant(encodings.reshape((1, seq_len, d_model)),\n",
    "                       dtype=tf.float32,\n",
    "                       name=\"positional_encodings\")\n",
    "\n",
    "\n",
    "\n",
    "def _prepare_embeddings(x, positional_encodings, keep_prob):\n",
    "    \"\"\"Add positional encoding and normalize embeddings.\n",
    "    \n",
    "    Args:\n",
    "        x: input embeddings of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        positional_encodings: encoding tensor of shape [1, SEQ_LEN, FEATURES].\n",
    "        keep_prob: The drop out probability.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"prepare_input\"):\n",
    "        _, seq_len, _ = x.shape\n",
    "        # TODO: put positional encoding back in\n",
    "#         x = x + positional_encodings[:, :seq_len, :]\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        #         return tf.contrib.layers.layer_norm(x)\n",
    "#         return _layer_norm(x)\n",
    "        # TODO: replace this back with _layer_norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:45:08.204044Z",
     "start_time": "2019-06-26T08:45:08.174237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000, 46)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.InteractiveSession()\n",
    "test = _generate_positional_encodings(46)\n",
    "test.eval().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:49:53.775669Z",
     "start_time": "2019-06-26T08:49:53.771858Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32  # 12000\n",
    "d_model = 128  # 512\n",
    "heads = 8\n",
    "keep_prob = 0.9\n",
    "n_layers = 2  # 6\n",
    "d_ff = 256  # 2048\n",
    "seq_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:49:57.357132Z",
     "start_time": "2019-06-26T08:49:55.653831Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input_embeddings = tf.random.uniform((batch_size,seq_length,d_model), dtype=tf.float32)\n",
    "input_embeddings = tf.placeholder(dtype=tf.float32, shape=(batch_size,seq_length,d_model), name=\"input\")\n",
    "\n",
    "fake_output = tf.placeholder(dtype=tf.float32, shape=(batch_size,seq_length), name=\"output\")\n",
    "\n",
    "positional_encodings = _generate_positional_encodings(d_model, seq_len=100)\n",
    "inputs_mask = tf.ones((1, 1, seq_length), dtype=float)\n",
    "input_embeddings = _prepare_embeddings(input_embeddings,\n",
    "                                          positional_encodings=positional_encodings,\n",
    "                                          keep_prob=keep_prob,\n",
    "                                          )\n",
    "encoding = _encoder(input_embeddings, mask=inputs_mask, n_layers=n_layers, heads=heads,\n",
    "                       keep_prob=keep_prob, d_ff=d_ff)\n",
    "\n",
    "reduced_mean = tf.math.reduce_mean(encoding, 2)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=fake_output, logits=reduced_mean)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "# g = tf.gradients(loss, [input_embeddings])\n",
    "random_variable = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)#[1:]\n",
    "# g = tf.gradients(loss, tf.trainable_variables())\n",
    "# g = tf.gradients(loss, random_variable)\n",
    "# g = tf.gradients(loss, random_variable)\n",
    "\n",
    "\n",
    "check_op = tf.add_check_numerics_ops()\n",
    "lr = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "#     training_op = optimizer.minimize(loss)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "#     grad_check = tf.check_numerics(training_op, \"NAN?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:49:59.193502Z",
     "start_time": "2019-06-26T08:49:59.182844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10, 128)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "x_batch = np.random.rand(batch_size,seq_length,d_model).astype(np.float)\n",
    "y_batch = np.random.rand(batch_size,seq_length).astype(np.float)\n",
    "x_batch.shape\n",
    "# x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:50:00.944889Z",
     "start_time": "2019-06-26T08:50:00.938858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:50:05.370611Z",
     "start_time": "2019-06-26T08:50:04.999968Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run([gradients], feed_dict={\n",
    "                       input_embeddings: x_batch,\n",
    "                       fake_output: y_batch\n",
    "                      })\n",
    "#     print(encoding.shape)\n",
    "#     eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T06:56:25.836209Z",
     "start_time": "2019-06-26T06:56:25.833875Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(output)#.shape\n",
    "# output.shape\n",
    "# np.isnan(output[0]).any()#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T07:06:14.950748Z",
     "start_time": "2019-06-26T07:06:14.927149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isnan(myarray).any() for myarray in output[0]]\n",
    "# [n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T03:51:23.439077Z",
     "start_time": "2019-06-26T03:51:23.434546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = tf.constant(4)\n",
    "# int(x)\n",
    "# input_embeddings\n",
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T06:40:21.821115Z",
     "start_time": "2019-06-26T06:40:21.816162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'prepare_input/norm/scale:0' shape=(128,) dtype=float32_ref>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)[0]#.op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T06:51:10.611127Z",
     "start_time": "2019-06-26T06:51:10.604951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder/attention_0/multi_head/query/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/query/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/key/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/key/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/value/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/value/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/attention/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/multi_head/attention/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/norm/scale:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_0/norm/base:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_0/feed_forward/hidden/kernel:0' shape=(128, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_0/feed_forward/hidden/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_0/feed_forward/out/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_0/feed_forward/out/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_0/norm/scale:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_0/norm/base:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/query/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/query/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/key/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/key/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/value/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/value/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/attention/kernel:0' shape=(128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/multi_head/attention/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/norm/scale:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/attention_1/norm/base:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_1/feed_forward/hidden/kernel:0' shape=(128, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_1/feed_forward/hidden/bias:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_1/feed_forward/out/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_1/feed_forward/out/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_1/norm/scale:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'encoder/ff_1/norm/base:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:51:54.011240Z",
     "start_time": "2019-06-26T08:51:54.007910Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, z = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T08:51:58.332121Z",
     "start_time": "2019-06-26T08:51:58.327819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
