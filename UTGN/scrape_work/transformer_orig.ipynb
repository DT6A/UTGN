{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T04:12:27.901966Z",
     "start_time": "2019-06-26T04:12:25.802957Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T04:21:12.818429Z",
     "start_time": "2019-06-26T04:21:12.576961Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mean_std(x: tf.Tensor):\n",
    "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    squared = tf.square(x - mean)\n",
    "    variance = tf.reduce_mean(squared, axis=-1, keepdims=True)\n",
    "    std = tf.sqrt(variance)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "def layer_norm(layer: tf.Tensor):\n",
    "    with tf.variable_scope(\"norm\"):\n",
    "        scale = tf.get_variable(\"scale\", shape=layer.shape[-1], dtype=tf.float32)\n",
    "        base = tf.get_variable(\"base\", shape=layer.shape[-1], dtype=tf.float32)\n",
    "        mean, std = get_mean_std(layer)\n",
    "        norm = (layer - mean) / (std + 1e-6)\n",
    "        return norm * scale + base\n",
    "\n",
    "def attention(query: tf.Tensor, key: tf.Tensor, value: tf.Tensor, *,\n",
    "              mask: tf.Tensor,\n",
    "              keep_prob: float):\n",
    "    d_k = query.shape[-1].value\n",
    "    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2]))\n",
    "    scores = scores / tf.constant(math.sqrt(d_k))\n",
    "    mask_add = ((scores * 0) - 1e9) * (tf.constant(1.) - mask)\n",
    "    scores = scores * mask + mask_add\n",
    "    attn = tf.nn.softmax(scores, axis=-1)\n",
    "    attn = tf.nn.dropout(attn, keep_prob)\n",
    "    return tf.matmul(attn, value), attn\n",
    "\n",
    "def prepare_for_multi_head_attention(x: tf.Tensor, heads: int, name: str):\n",
    "    n_batches, seq_len, d_model = x.shape\n",
    "    assert d_model % heads == 0\n",
    "    d_k = d_model // heads\n",
    "    x = tf.layers.dense(x, units=d_model, name=name)\n",
    "    x = tf.reshape(x, shape=[n_batches, seq_len, heads, d_k])\n",
    "    x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    return x\n",
    "\n",
    "def multi_head_attention(query: tf.Tensor, key: tf.Tensor, value: tf.Tensor, *,\n",
    "                         mask: tf.Tensor,\n",
    "                         heads: int,\n",
    "                         keep_prob: float):\n",
    "    with tf.variable_scope(\"multi_head\"):\n",
    "        n_batches, seq_len, d_model = query.shape\n",
    "        query = prepare_for_multi_head_attention(query, heads, \"query\")\n",
    "        key = prepare_for_multi_head_attention(key, heads, \"key\")\n",
    "        value = prepare_for_multi_head_attention(value, heads, \"value\")\n",
    "        mask = tf.expand_dims(mask, axis=1)\n",
    "        out, _ = attention(query, key, value, mask=mask, keep_prob=keep_prob)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        out = tf.reshape(out, shape=[n_batches, seq_len, d_model])\n",
    "        return tf.layers.dense(out, units=d_model, name=\"attention\")\n",
    "    \n",
    "def feed_forward(x: tf.Tensor,\n",
    "                 d_model: int, d_ff: int, keep_prob: float):\n",
    "    \n",
    "    with tf.variable_scope(\"feed_forward\"):\n",
    "        hidden = tf.layers.dense(x, units=d_ff, name=\"hidden\")\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        hidden = tf.nn.dropout(hidden, keep_prob=keep_prob)\n",
    "        return tf.layers.dense(hidden, units=d_model, name=\"out\")\n",
    "    \n",
    "def encoder_layer(x: tf.Tensor, *,\n",
    "                  mask: tf.Tensor, index: int, heads: int,\n",
    "                  keep_prob: float, d_ff: int):\n",
    "    d_model = x.shape[-1]\n",
    "\n",
    "    with tf.variable_scope(f\"attention_{index}\"):\n",
    "        attention_out = multi_head_attention(x, x, x,\n",
    "                                             mask=mask, heads=heads, keep_prob=keep_prob)\n",
    "        added = x + tf.nn.dropout(attention_out, keep_prob)\n",
    "        x = layer_norm(added)\n",
    "    with tf.variable_scope(f\"ff_{index}\"):\n",
    "        ff_out = feed_forward(x, d_model, d_ff, keep_prob)\n",
    "        added = x + tf.nn.dropout(ff_out, keep_prob)\n",
    "        return layer_norm(added)\n",
    "\n",
    "def encoder(x: tf.Tensor, *,\n",
    "            mask: tf.Tensor,\n",
    "            n_layers: int,\n",
    "            heads: int, keep_prob: float, d_ff: int):\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        for i in range(n_layers):\n",
    "            x = encoder_layer(x,\n",
    "                              mask=mask, index=i,\n",
    "                              heads=heads, keep_prob=keep_prob, d_ff=d_ff)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def decoder_layer(encoding: tf.Tensor, x: tf.Tensor, *,\n",
    "                  enc_mask: tf.Tensor, mask: tf.Tensor,\n",
    "                  index: int, heads: int, keep_prob: float, d_ff: int):\n",
    "    d_model = encoding.shape[-1]\n",
    "    \n",
    "    with tf.variable_scope(f\"{index}_self_attention\"):\n",
    "        attention_out = multi_head_attention(x, x, x,\n",
    "                                             mask=mask, heads=heads, keep_prob=keep_prob)\n",
    "        added = x + tf.nn.dropout(attention_out, keep_prob=keep_prob)\n",
    "        x = layer_norm(added)\n",
    "    with tf.variable_scope(f\"{index}_encoding_attention\"):\n",
    "        attention_out = multi_head_attention(x, encoding, encoding,\n",
    "                                             mask=enc_mask, heads=heads, keep_prob=keep_prob)\n",
    "        \n",
    "        added = x + tf.nn.dropout(attention_out, keep_prob=keep_prob)\n",
    "        x = layer_norm(added)\n",
    "    with tf.variable_scope(f\"{index}_ff\"):\n",
    "        ff_out = feed_forward(x, d_model, d_ff, keep_prob)\n",
    "        \n",
    "        added = x + tf.nn.dropout(ff_out, keep_prob)\n",
    "        return layer_norm(added)\n",
    "\n",
    "def decoder(encoding: tf.Tensor, x: tf.Tensor, *,\n",
    "            enc_mask: tf.Tensor, mask: tf.Tensor,\n",
    "            n_layers: int,\n",
    "            heads: int, keep_prob: float, d_ff: int):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        for i in range(n_layers):\n",
    "            x = decoder_layer(encoding, x,\n",
    "                              enc_mask=enc_mask, mask=mask, index=i,\n",
    "                              heads=heads, keep_prob=keep_prob, d_ff=d_ff)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def get_embeddings(input_ids: tf.Tensor, output_ids: tf.Tensor,\n",
    "                   vocab_size: int, d_model: int):\n",
    "\n",
    "    word_embeddings = tf.get_variable(\"word_embeddings\",\n",
    "                                      shape=[vocab_size, d_model],\n",
    "                                      dtype=tf.float32,\n",
    "                                      initializer=tf.initializers.random_normal())\n",
    "    in_emb = tf.nn.embedding_lookup(word_embeddings, input_ids)\n",
    "    out_emb = tf.nn.embedding_lookup(word_embeddings, output_ids)\n",
    "    return word_embeddings, in_emb, out_emb\n",
    "\n",
    "def generate_positional_encodings(d_model: int, max_len: int = 5000):\n",
    "    encodings = np.zeros((max_len, d_model), dtype=float)\n",
    "    position = np.arange(0, max_len).reshape((max_len, 1))\n",
    "    two_i = np.arange(0, d_model, 2)\n",
    "    div_term = np.exp(-math.log(10000.0) * two_i / d_model)\n",
    "    encodings[:, 0::2] = np.sin(position * div_term)\n",
    "    encodings[:, 1::2] = np.cos(position * div_term)\n",
    "    return tf.constant(encodings.reshape((1, max_len, d_model)),\n",
    "                       dtype=tf.float32, name=\"positional_encodings\")\n",
    "\n",
    "def prepare_embeddings(x: tf.Tensor, *,\n",
    "                       positional_encodings: tf.Tensor,\n",
    "                       keep_prob: float, is_input: bool):\n",
    "    name = \"prepare_input\" if is_input else \"prepare_output\"\n",
    "    with tf.variable_scope(name):\n",
    "        _, seq_len, _ = x.shape\n",
    "        x = x + positional_encodings[:, :seq_len, :]\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        return layer_norm(x)\n",
    "\n",
    "def generator(x: tf.Tensor, *, vocab_size: int):\n",
    "#\n",
    "    res = tf.layers.dense(x, units=vocab_size, name=\"generator\")\n",
    "    return tf.nn.log_softmax(res, axis=-1)\n",
    "\n",
    "\n",
    "def label_smoothing_loss(results: tf.Tensor, expected: tf.Tensor, *,\n",
    "                         vocab_size: int, smoothing: float):\n",
    "    results = tf.reshape(results, shape=(-1, vocab_size))\n",
    "    expected = tf.reshape(expected, shape=[-1])\n",
    "\n",
    "    confidence = 1 - smoothing\n",
    "    smoothing = smoothing / (vocab_size - 1)\n",
    "    expected = tf.one_hot(expected, depth=vocab_size) * (confidence - smoothing)\n",
    "    expected += smoothing\n",
    "\n",
    "    results = tf.distributions.Categorical(logits=results)\n",
    "    expected = tf.distributions.Categorical(logits=expected)\n",
    "    return tf.reduce_mean(tf.distributions.kl_divergence(results, expected))\n",
    "\n",
    "def generate_data(batch_size: int, seq_len: int, vocab_size: int):\n",
    "    start_token = vocab_size - 1\n",
    "    repeat_token = vocab_size - 2\n",
    "    vocab_size -= 2\n",
    "\n",
    "    inputs = np.random.randint(0, vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "    outputs = np.zeros((batch_size, seq_len + 1), dtype=int)\n",
    "    outputs[:, 1:] = np.flip(inputs, 1)\n",
    "    outputs[:, 0] = start_token\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        v = np.zeros(vocab_size, dtype=bool)\n",
    "        for j in range(seq_len):\n",
    "            word = inputs[i, j]\n",
    "            if v[word]:\n",
    "                v[word] = False\n",
    "                outputs[i][seq_len - j] = repeat_token\n",
    "            else:\n",
    "                v[word] = True\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def noam_learning_rate(step: int, warm_up: float, d_model: int):\n",
    "    return (d_model ** -.5) * min(step ** -.5, step * warm_up ** -1.5)\n",
    "\n",
    "def output_subsequent_mask(seq_len: int):\n",
    "    mask = np.zeros((seq_len, seq_len), dtype=float)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1):\n",
    "            mask[i, j] = 1.\n",
    "\n",
    "    return mask\n",
    "\n",
    "def train():\n",
    "    seq_length = 10\n",
    "    vocab_size = 10 + 1 + 1\n",
    "    vocab_str = [f\"{i}\" for i in range(10)]\n",
    "    vocab_str += ['X', 'S']\n",
    "    \n",
    "    batch_size = 32  # 12000\n",
    "    d_model = 128  # 512\n",
    "    heads = 8\n",
    "    keep_prob = 0.9\n",
    "    n_layers = 2  # 6\n",
    "    d_ff = 256  # 2048\n",
    "    positional_encodings = generate_positional_encodings(d_model)\n",
    "    inputs = tf.placeholder(dtype=tf.int32,\n",
    "                            shape=(batch_size, seq_length), name=\"input\")\n",
    "    outputs = tf.placeholder(dtype=tf.int32,\n",
    "                             shape=(batch_size, seq_length), name=\"output\")\n",
    "    expected = tf.placeholder(dtype=tf.int32,\n",
    "                              shape=(batch_size, seq_length), name=\"expected\")\n",
    "    inputs_mask = tf.placeholder(dtype=tf.float32,\n",
    "                                 shape=(1, 1, seq_length),\n",
    "                                 name=\"input_mask\")\n",
    "    output_mask = tf.placeholder(dtype=tf.float32,\n",
    "                                 shape=(1, seq_length, seq_length),\n",
    "                                 name=\"output_mask\")\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name=\"learning_rate\")\n",
    "    w_embed, input_embeddings, output_embeddings = get_embeddings(inputs, outputs, vocab_size,\n",
    "                                                                  d_model)\n",
    "    input_embeddings = prepare_embeddings(input_embeddings,\n",
    "                                          positional_encodings=positional_encodings,\n",
    "                                          keep_prob=keep_prob,\n",
    "                                          is_input=True)\n",
    "    output_embeddings = prepare_embeddings(output_embeddings,\n",
    "                                           positional_encodings=positional_encodings,\n",
    "                                           keep_prob=keep_prob,\n",
    "                                           is_input=False)\n",
    "\n",
    "    encoding = encoder(input_embeddings, mask=inputs_mask, n_layers=n_layers, heads=heads,\n",
    "                       keep_prob=keep_prob, d_ff=d_ff)\n",
    "    decoding = decoder(encoding, output_embeddings,\n",
    "                       enc_mask=inputs_mask, mask=output_mask,\n",
    "                       n_layers=n_layers, heads=heads, keep_prob=keep_prob, d_ff=d_ff)\n",
    "    log_results = generator(decoding, vocab_size=vocab_size)\n",
    "    results = tf.exp(log_results)\n",
    "    loss = label_smoothing_loss(log_results, expected, vocab_size=vocab_size, smoothing=0.0)\n",
    "    adam = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-5)\n",
    "    params = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, params), 5.)\n",
    "    grads_and_vars = list(zip(grads, params))\n",
    "    train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")\n",
    "\n",
    "    warm_up = 400\n",
    "    batch_in_mask = np.ones((1, 1, seq_length), dtype=float)\n",
    "    batch_out_mask = output_subsequent_mask(seq_length)\n",
    "    batch_out_mask = batch_out_mask.reshape(1, seq_length, seq_length)\n",
    "    def __print_seq(seq):\n",
    "        return ' '.join([vocab_str[i] for i in seq])\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(100_000):\n",
    "            lr = noam_learning_rate(i + 1, warm_up, d_model)\n",
    "            \n",
    "            batch_in, batch_out = generate_data(batch_size, seq_length, vocab_size)\n",
    "            _, batch_loss, batch_res = session.run([train_op, loss, results],\n",
    "                                                   feed_dict={\n",
    "                                                       learning_rate: lr,\n",
    "                                                       inputs: batch_in,\n",
    "                                                       outputs: batch_out[:, :-1],\n",
    "                                                       expected: batch_out[:, 1:],\n",
    "                                                       inputs_mask: batch_in_mask,\n",
    "                                                       output_mask: batch_out_mask\n",
    "                                                   })\n",
    "            if i % 100 == 0:\n",
    "                print(f\"step={i}\\tloss={batch_loss: .6f}\")\n",
    "                print(f\"inp=  {__print_seq(batch_in[0])}\")\n",
    "                print(f\"exp={__print_seq(batch_out[0])}\")\n",
    "                print(f\"res=  {__print_seq(np.argmax(batch_res[0], -1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T04:22:36.949602Z",
     "start_time": "2019-06-26T04:21:17.593060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-c73f9d214c14>:152: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-3-c73f9d214c14>:33: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-c73f9d214c14>:171: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/distributions/categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From <ipython-input-3-c73f9d214c14>:173: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "step=0\tloss= 0.064222\n",
      "inp=  2 8 1 2 7 1 0 7 3 6\n",
      "exp=S 6 3 X 0 X 7 X 1 8 2\n",
      "res=  2 4 2 2 8 4 4 4 4 2\n",
      "step=100\tloss= 0.048881\n",
      "inp=  3 9 7 2 2 6 1 8 3 0\n",
      "exp=S 0 X 8 1 6 X 2 7 9 3\n",
      "res=  X X X X X X X X X X\n",
      "step=200\tloss= 0.047613\n",
      "inp=  3 4 1 9 3 9 4 0 9 7\n",
      "exp=S 7 9 0 X X X 9 1 4 3\n",
      "res=  X X X X X X X X 7 X\n",
      "step=300\tloss= 0.046330\n",
      "inp=  1 8 1 6 6 1 9 3 4 4\n",
      "exp=S X 4 3 9 1 X 6 X 8 1\n",
      "res=  X X X X X X 1 1 1 6\n",
      "step=400\tloss= 0.044737\n",
      "inp=  6 0 4 5 5 6 0 5 4 2\n",
      "exp=S 2 X 5 X X X 5 4 0 6\n",
      "res=  X X X X X 6 6 6 6 6\n",
      "step=500\tloss= 0.043971\n",
      "inp=  6 8 0 1 1 7 6 4 2 2\n",
      "exp=S X 2 4 X 7 X 1 0 8 6\n",
      "res=  X X X X X X 6 6 6 6\n",
      "step=600\tloss= 0.041903\n",
      "inp=  8 7 4 2 0 5 9 9 0 5\n",
      "exp=S X X X 9 5 0 2 4 7 8\n",
      "res=  X X X 0 X X 4 7 8 8\n",
      "step=700\tloss= 0.041890\n",
      "inp=  5 6 0 2 5 3 8 4 9 6\n",
      "exp=S X 9 4 8 3 X 2 0 6 5\n",
      "res=  X X X X X X 6 6 6 5\n",
      "step=800\tloss= 0.042175\n",
      "inp=  0 5 5 6 1 5 6 3 7 9\n",
      "exp=S 9 7 3 X 5 1 6 X 5 0\n",
      "res=  X X X X X X X X 5 7\n",
      "step=900\tloss= 0.039933\n",
      "inp=  4 7 6 7 5 6 0 1 8 4\n",
      "exp=S X 8 1 0 X 5 X 6 7 4\n",
      "res=  X X X X X 6 6 6 7 4\n",
      "step=1000\tloss= 0.041106\n",
      "inp=  8 7 3 6 4 3 2 1 0 3\n",
      "exp=S 3 0 1 2 X 4 6 3 7 8\n",
      "res=  X X X X X X X X X 8\n",
      "step=1100\tloss= 0.040254\n",
      "inp=  9 6 4 1 2 7 2 5 9 3\n",
      "exp=S 3 X 5 X 7 2 1 4 6 9\n",
      "res=  X X X X X X 9 9 6 9\n",
      "step=1200\tloss= 0.039850\n",
      "inp=  5 7 9 0 0 9 2 5 8 2\n",
      "exp=S X 8 X 2 X X 0 9 7 5\n",
      "res=  X X X X 9 5 9 9 5 5\n",
      "step=1300\tloss= 0.038383\n",
      "inp=  6 3 4 9 7 4 6 8 6 3\n",
      "exp=S X 6 8 X X 7 9 4 3 6\n",
      "res=  X X X X X 9 9 3 3 6\n",
      "step=1400\tloss= 0.040215\n",
      "inp=  4 9 1 7 6 8 4 4 9 1\n",
      "exp=S X X 4 X 8 6 7 1 9 4\n",
      "res=  X X X X X 9 1 9 9 4\n",
      "step=1500\tloss= 0.039785\n",
      "inp=  8 1 4 0 4 6 0 7 9 7\n",
      "exp=S X 9 7 X 6 X 0 4 1 8\n",
      "res=  X X X X X X 4 4 8 8\n",
      "step=1600\tloss= 0.039016\n",
      "inp=  6 6 3 6 5 9 7 8 0 7\n",
      "exp=S X 0 8 7 9 5 6 3 X 6\n",
      "res=  X X X X X X X X X 6\n",
      "step=1700\tloss= 0.039485\n",
      "inp=  6 6 4 8 2 8 2 0 0 4\n",
      "exp=S X X 0 X X 2 8 4 X 6\n",
      "res=  X X X X X 8 4 4 6 6\n",
      "step=1800\tloss= 0.038054\n",
      "inp=  3 4 7 9 2 3 3 7 0 0\n",
      "exp=S X 0 X 3 X 2 9 7 4 3\n",
      "res=  X X X X X 7 7 7 4 3\n",
      "step=1900\tloss= 0.038847\n",
      "inp=  8 8 2 4 8 5 9 2 8 9\n",
      "exp=S X X X 9 5 8 4 2 X 8\n",
      "res=  X X X X X X X X X 8\n",
      "step=2000\tloss= 0.038101\n",
      "inp=  4 6 1 1 0 1 6 3 7 8\n",
      "exp=S 8 7 3 X 1 0 X 1 6 4\n",
      "res=  X X X X X X X 6 6 4\n",
      "step=2100\tloss= 0.036599\n",
      "inp=  3 4 7 8 0 3 7 6 9 2\n",
      "exp=S 2 9 6 X X 0 8 7 4 3\n",
      "res=  X X X X X 7 7 7 4 3\n",
      "step=2200\tloss= 0.038529\n",
      "inp=  8 0 4 2 6 9 9 7 5 4\n",
      "exp=S X 5 7 X 9 6 2 4 0 8\n",
      "res=  X X X X 4 4 4 4 0 8\n",
      "step=2300\tloss= 0.037893\n",
      "inp=  2 9 4 0 8 7 5 0 1 2\n",
      "exp=S X 1 X 5 7 8 0 4 9 2\n",
      "res=  X X X X X X 9 4 9 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-c73f9d214c14>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m                                                        \u001b[0mexpected\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                                                        \u001b[0minputs_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_in_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                                                        \u001b[0moutput_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_out_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                                                    })\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
