{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T02:48:58.922624Z",
     "start_time": "2019-07-11T02:48:56.315925Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Encoder portion of the transformer.\n",
    "\n",
    "Input tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES]\n",
    "Output tensor of the same shape.\n",
    "\n",
    "# TODO: Take a look at _higher_recurrence and perform similar\n",
    "functions (like updating config, etc)\n",
    "# TODO: add trainable parameters to collection \n",
    "(check _recurrence)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "cast32 = lambda x: tf.dtypes.cast(x, tf.float32)\n",
    "none_to1 = lambda x: -1 if x == None else x\n",
    "\n",
    "\n",
    "def _get_mean_std(x):\n",
    "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    squared = tf.square(x - mean)\n",
    "    variance = tf.reduce_mean(squared, axis=-1, keepdims=True)\n",
    "    std = tf.sqrt(variance)\n",
    "    return mean, std\n",
    "\n",
    "def _layer_norm(layer):\n",
    "    \"\"\"Perform layer normalization.\n",
    "\n",
    "    Not the same as batch normalization.\n",
    "\n",
    "    Args:\n",
    "        layer: Tensor\n",
    "\n",
    "    Returns:\n",
    "        Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"norm\"):\n",
    "        scale = tf.get_variable(\n",
    "            \"scale\", \n",
    "            shape=layer.shape[-1], \n",
    "            dtype=tf.float32)\n",
    "        base = tf.get_variable(\n",
    "            \"base\", \n",
    "            shape=layer.shape[-1], \n",
    "            dtype=tf.float32)\n",
    "        mean, std = _get_mean_std(layer)\n",
    "        norm = (layer - mean) / (std + 1e-6)\n",
    "        return norm * scale + base\n",
    "    \n",
    "    \n",
    "def _attention(query, key, value, mask, keep_prob, train=True):\n",
    "    \"\"\"Calculates scaled dot-product attention.\n",
    "    \n",
    "    softmax(Q K^{T} / sqrt(d_{k}))V\n",
    "    \n",
    "    Args:\n",
    "        query: A query tensor of shape [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES].\n",
    "        key:  The key tensor.\n",
    "        value: The value tensor.\n",
    "        mask: Mask of shape [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES]\n",
    "        keep_prob: The drop out probability.\n",
    "        train: train or predict\n",
    "    \n",
    "    Returns:\n",
    "        The scaled dot-product attention.\n",
    "        Shape: [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES]\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = query.shape[-1].value\n",
    "    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2]))\n",
    "    scores = scores / tf.constant(np.sqrt(d_k), dtype=tf.float32)\n",
    "    mask_add = ((scores * 0) - cast32(1e9)) * (tf.constant(1.) - cast32(mask))\n",
    "    scores = scores * cast32(mask) + mask_add\n",
    "    attn = tf.nn.softmax(scores, axis=-1)\n",
    "    if train:\n",
    "        attn = tf.nn.dropout(attn, keep_prob)\n",
    "    return tf.matmul(attn, value)\n",
    "\n",
    "\n",
    "def _prepare_multi_head_attention(x, heads, name):\n",
    "    \"\"\"Prepares for multihead attention.\n",
    "    \n",
    "    Prepares query, key, value that have form [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor input.\n",
    "        heads: Number of heads.\n",
    "        name: Either query, key, or value.\n",
    "    \n",
    "    Returns:\n",
    "        A prepared Q, K, or V of form [BATCH_SIZE, HEADS, SEQ_LEN, FEATURES]\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: Dimension of features must be divisible by the number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    n_batches, seq_len, d_model = x.get_shape().as_list()\n",
    "    seq_len = none_to1(seq_len)\n",
    "    assert d_model % heads == 0, \"Dimension of features needs to be divisible by the number of heads.\"\n",
    "    d_k = d_model // heads\n",
    "    x = tf.layers.dense(x, units=d_model, name=name)\n",
    "    x = tf.reshape(x, shape=(n_batches, seq_len, heads, d_k))\n",
    "    x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _multi_head_attention(query, key, value, mask, heads, keep_prob, train=True):\n",
    "    \"\"\"Calculates the multihead attention.\n",
    "    \n",
    "    Args:\n",
    "        query: query tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        key: key tensor.\n",
    "        value: value tensor.\n",
    "        mask: mask tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        heads: number of heads.\n",
    "        keep_prob: The drop out probability.\n",
    "        train: train or predict\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES]\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"multi_head\"):\n",
    "        n_batches, seq_len, d_model = query.get_shape().as_list()\n",
    "        query = _prepare_multi_head_attention(query, heads, \"query\")\n",
    "        key = _prepare_multi_head_attention(key, heads, \"key\")\n",
    "        value = _prepare_multi_head_attention(value, heads, \"value\")\n",
    "        mask = tf.expand_dims(mask, axis=1)\n",
    "        out = _attention(\n",
    "            query, \n",
    "            key, \n",
    "            value, \n",
    "            mask=mask, \n",
    "            keep_prob=keep_prob,\n",
    "            train=train)\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        seq_len = none_to1(seq_len)\n",
    "        out = tf.reshape(out, shape=[n_batches, seq_len, d_model])\n",
    "        return tf.layers.dense(out, units=d_model, name=\"attention\")\n",
    "\n",
    "\n",
    "def _feed_forward(x, d_model, d_ff, keep_prob, train=True):\n",
    "    \"\"\"Feed forward layer along with of relu and dropout.\n",
    "    \n",
    "    FFN(x) = max(0,xW1+b1)W2+b2\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor.\n",
    "        d_model: dimension of W2.\n",
    "        d_ff: dimension of W1.\n",
    "        keep_prob: The drop out probability.\n",
    "        train: train or predict\n",
    "        \n",
    "    Returns:\n",
    "        Tensor\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"feed_forward\"):\n",
    "        hidden = tf.layers.dense(x, units=d_ff, name=\"hidden\")\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, keep_prob=keep_prob)\n",
    "        return tf.layers.dense(hidden, units=d_model, name=\"out\")\n",
    "\n",
    "\n",
    "def _encoder_layer(x, mask, layer_num, \n",
    "                   heads, keep_prob, d_ff, \n",
    "                   train=True):\n",
    "    \"\"\"Create a single encoder layer.\n",
    "    \n",
    "    Args:\n",
    "        x: input tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        mask: mask tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        layer_num: The number label of an encoder layer.\n",
    "        heads: Number of heads.\n",
    "        keep_prob: The drop out probability.\n",
    "        d_ff: dimension of W1.\n",
    "        train: train or predict\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \"\"\"\n",
    "\n",
    "    d_model = x.shape[-1]\n",
    "    # with tf.variable_scope(f\"attention_{layer_num}\"):\n",
    "    with tf.variable_scope(\"attention_\" + str(layer_num)):\n",
    "        attention_out = _multi_head_attention(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            mask=mask,\n",
    "            heads=heads,\n",
    "            keep_prob=keep_prob,\n",
    "            train=train)\n",
    "\n",
    "        if train:\n",
    "            attention_out = tf.nn.dropout(\n",
    "                attention_out, \n",
    "                keep_prob)\n",
    "        added = x + attention_out\n",
    "        x = _layer_norm(added)\n",
    "\n",
    "    # with tf.variable_scope(f\"ff_{layer_num}\"):\n",
    "    with tf.variable_scope(\"ff_\" + str(layer_num)):\n",
    "        ff_out = _feed_forward(x, d_model, d_ff, keep_prob, train=train)\n",
    "        if train:\n",
    "            ff_out = tf.nn.dropout(ff_out, keep_prob)\n",
    "        added = x + ff_out\n",
    "        return _layer_norm(added)\n",
    "\n",
    "\n",
    "def _encoder(x, mask, n_layers, heads, keep_prob, d_ff, train=True):\n",
    "    \"\"\"Create the encoder architecture\n",
    "    \n",
    "    Args:\n",
    "        x: input tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        mask: mask tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        n_layers: number of layers of the encoder model.\n",
    "        heads: number of heads.\n",
    "        keep_prob: The drop out probability.\n",
    "        d_ff: dimension of W1.\n",
    "        train: train or predict\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape: [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        for i in range(n_layers):\n",
    "            x = _encoder_layer(\n",
    "                x,\n",
    "                mask=mask,\n",
    "                layer_num=i,\n",
    "                heads=heads,\n",
    "                keep_prob=keep_prob,\n",
    "                d_ff=d_ff,\n",
    "                train=train)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _generate_positional_encodings(d_model, seq_len=5000):\n",
    "    \"\"\"Create positional encoding.\n",
    "    \n",
    "    Args:\n",
    "        d_model: dimension of input embeddings\n",
    "        seq_len: maximum sequence length of batch\n",
    "        \n",
    "    Returns:\n",
    "        Constant tensor of shape [1, seq_len, d_model]\n",
    "    \"\"\"\n",
    "\n",
    "    encodings = np.zeros((seq_len, d_model), dtype=float)\n",
    "    position = np.arange(0, seq_len).reshape((seq_len, 1))\n",
    "    two_i = np.arange(0, d_model, 2)\n",
    "    div_term = np.exp(-np.log(10000.0) * two_i / d_model)\n",
    "    encodings[:, 0::2] = np.sin(position * div_term)\n",
    "    encodings[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "    pos_encodings = tf.constant(\n",
    "        encodings.reshape((1, seq_len, d_model)),\n",
    "        dtype=tf.float32,\n",
    "        name=\"positional_encodings\")\n",
    "\n",
    "    return pos_encodings\n",
    "\n",
    "\n",
    "def _prepare_embeddings(x, positional_encodings, \n",
    "                        keep_prob, train=True):\n",
    "    \"\"\"Add positional encoding and normalize embeddings.\n",
    "    \n",
    "    Args:\n",
    "        x: input embeddings of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "        positional_encodings: encoding tensor of shape [1, SEQ_LEN, FEATURES].\n",
    "        keep_prob: The drop out probability.\n",
    "        train: train or predict\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape [BATCH_SIZE, SEQ_LEN, FEATURES].\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"prepare_input\"):\n",
    "        _, seq_len, _ = x.shape\n",
    "        # TODO: put positional encoding back in\n",
    "        # x = x + positional_encodings[:, :seq_len, :]\n",
    "\n",
    "        if train:\n",
    "            x = tf.nn.dropout(x, keep_prob)\n",
    "        return _layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T03:00:41.385976Z",
     "start_time": "2019-07-11T03:00:41.333899Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def decoder_layer(encoding: tf.Tensor, x: tf.Tensor, *,\n",
    "                  enc_mask: tf.Tensor, mask: tf.Tensor,\n",
    "                  index: int, heads: int, keep_prob: float, d_ff: int):\n",
    "    d_model = encoding.shape[-1]\n",
    "    \n",
    "    with tf.variable_scope(f\"{index}_self_attention\"):\n",
    "        attention_out = _multi_head_attention(x, x, x,\n",
    "                                             mask=mask, heads=heads, keep_prob=keep_prob)\n",
    "        added = x + tf.nn.dropout(attention_out, keep_prob=keep_prob)\n",
    "        x = _layer_norm(added)\n",
    "    with tf.variable_scope(f\"{index}_encoding_attention\"):\n",
    "        attention_out = _multi_head_attention(x, encoding, encoding,\n",
    "                                             mask=enc_mask, heads=heads, keep_prob=keep_prob)\n",
    "        \n",
    "        added = x + tf.nn.dropout(attention_out, keep_prob=keep_prob)\n",
    "        x = _layer_norm(added)\n",
    "    with tf.variable_scope(f\"{index}_ff\"):\n",
    "        ff_out = _feed_forward(x, d_model, d_ff, keep_prob)\n",
    "        \n",
    "        added = x + tf.nn.dropout(ff_out, keep_prob)\n",
    "        return _layer_norm(added)\n",
    "\n",
    "def decoder(encoding: tf.Tensor, x: tf.Tensor, *,\n",
    "            enc_mask: tf.Tensor, mask: tf.Tensor,\n",
    "            n_layers: int,\n",
    "            heads: int, keep_prob: float, d_ff: int):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        for i in range(n_layers):\n",
    "            x = decoder_layer(encoding, x,\n",
    "                              enc_mask=enc_mask, mask=mask, index=i,\n",
    "                              heads=heads, keep_prob=keep_prob, d_ff=d_ff)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def get_embeddings(input_ids: tf.Tensor, output_ids: tf.Tensor,\n",
    "                   vocab_size: int, d_model: int):\n",
    "\n",
    "    word_embeddings = tf.get_variable(\"word_embeddings\",\n",
    "                                      shape=[vocab_size, d_model],\n",
    "                                      dtype=tf.float32,\n",
    "                                      initializer=tf.initializers.random_normal())\n",
    "    in_emb = tf.nn.embedding_lookup(word_embeddings, input_ids)\n",
    "    out_emb = tf.nn.embedding_lookup(word_embeddings, output_ids)\n",
    "    return word_embeddings, in_emb, out_emb\n",
    "\n",
    "def generate_positional_encodings(d_model: int, max_len: int = 5000):\n",
    "    encodings = np.zeros((max_len, d_model), dtype=float)\n",
    "    position = np.arange(0, max_len).reshape((max_len, 1))\n",
    "    two_i = np.arange(0, d_model, 2)\n",
    "    div_term = np.exp(-math.log(10000.0) * two_i / d_model)\n",
    "    encodings[:, 0::2] = np.sin(position * div_term)\n",
    "    encodings[:, 1::2] = np.cos(position * div_term)\n",
    "    return tf.constant(encodings.reshape((1, max_len, d_model)),\n",
    "                       dtype=tf.float32, name=\"positional_encodings\")\n",
    "\n",
    "def prepare_embeddings(x: tf.Tensor, *,\n",
    "                       positional_encodings: tf.Tensor,\n",
    "                       keep_prob: float, is_input: bool):\n",
    "    name = \"prepare_input\" if is_input else \"prepare_output\"\n",
    "    with tf.variable_scope(name):\n",
    "        _, seq_len, _ = x.shape\n",
    "        x = x + positional_encodings[:, :seq_len, :]\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        return _layer_norm(x)\n",
    "\n",
    "def generator(x: tf.Tensor, *, vocab_size: int):\n",
    "#\n",
    "    res = tf.layers.dense(x, units=vocab_size, name=\"generator\")\n",
    "    return tf.nn.log_softmax(res, axis=-1)\n",
    "\n",
    "\n",
    "def label_smoothing_loss(results: tf.Tensor, expected: tf.Tensor, *,\n",
    "                         vocab_size: int, smoothing: float):\n",
    "    results = tf.reshape(results, shape=(-1, vocab_size))\n",
    "    expected = tf.reshape(expected, shape=[-1])\n",
    "\n",
    "    confidence = 1 - smoothing\n",
    "    smoothing = smoothing / (vocab_size - 1)\n",
    "    expected = tf.one_hot(expected, depth=vocab_size) * (confidence - smoothing)\n",
    "    expected += smoothing\n",
    "\n",
    "    results = tf.distributions.Categorical(logits=results)\n",
    "    expected = tf.distributions.Categorical(logits=expected)\n",
    "    return tf.reduce_mean(tf.distributions.kl_divergence(results, expected))\n",
    "\n",
    "def generate_data(batch_size: int, seq_len: int, vocab_size: int):\n",
    "    start_token = vocab_size - 1\n",
    "    repeat_token = vocab_size - 2\n",
    "    vocab_size -= 2\n",
    "\n",
    "    inputs = np.random.randint(0, vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "    outputs = np.zeros((batch_size, seq_len + 1), dtype=int)\n",
    "    outputs[:, 1:] = np.flip(inputs, 1)\n",
    "    outputs[:, 0] = start_token\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        v = np.zeros(vocab_size, dtype=bool)\n",
    "        for j in range(seq_len):\n",
    "            word = inputs[i, j]\n",
    "            if v[word]:\n",
    "                v[word] = False\n",
    "                outputs[i][seq_len - j] = repeat_token\n",
    "            else:\n",
    "                v[word] = True\n",
    "\n",
    "    return inputs, outputs\n",
    "#     return inputs, inputs\n",
    "\n",
    "def noam_learning_rate(step: int, warm_up: float, d_model: int):\n",
    "    return (d_model ** -.5) * min(step ** -.5, step * warm_up ** -1.5)\n",
    "\n",
    "def output_subsequent_mask(seq_len: int):\n",
    "    mask = np.zeros((seq_len, seq_len), dtype=float)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i + 1):\n",
    "            mask[i, j] = 1.\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T06:27:45.716256Z",
     "start_time": "2019-07-13T06:27:45.703794Z"
    }
   },
   "outputs": [],
   "source": [
    "def ut_function(state,\n",
    "                step,\n",
    "                halting_probability,\n",
    "                remainders,\n",
    "                n_updates,\n",
    "                previous_state,\n",
    "                encoder_layer,\n",
    "                config={}):\n",
    "    \"\"\"Implements ACT (position-wise halting).\n",
    "    \n",
    "    Args:\n",
    "        state: Tensor of shape [batch_size, length, input_dim]\n",
    "        step: indicates number of steps taken so far\n",
    "        halting_probability: halting probability\n",
    "        remainders: ACT remainders\n",
    "        n_updates: ACT n_updates\n",
    "        previous_state: previous state\n",
    "        encoder_layer: encoder layer function\n",
    "        config: configuration dict\n",
    "      \n",
    "    Returns:\n",
    "        transformed_state: transformed state\n",
    "        step: step + 1\n",
    "        halting_probability: halting probability\n",
    "        remainders: act remainders\n",
    "        n_updates: act n_updates\n",
    "        new_state: new state\n",
    "        \n",
    "    TODO: include positional encodings\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(\"sigmoid_activation_for_pondering\"):\n",
    "        p = tf.layers.dense(state, 1, activation=tf.nn.sigmoid, use_bias=True)\n",
    "\n",
    "    # Mask for inputs which have not halted yet\n",
    "    still_running = tf.cast(tf.less(halting_probability, 1.0), tf.float32)\n",
    "\n",
    "    # Mask of inputs which halted at this step\n",
    "    new_halted = tf.cast(\n",
    "        tf.greater(halting_probability + p * still_running, threshold),\n",
    "        tf.float32) * still_running\n",
    "\n",
    "    # Mask of inputs which haven't halted, and didn't halt this step\n",
    "    still_running = tf.cast(\n",
    "        tf.less_equal(halting_probability + p * still_running, threshold),\n",
    "        tf.float32) * still_running\n",
    "\n",
    "    # Add the halting probability for this step to the halting\n",
    "    # probabilities for those input which haven't halted yet\n",
    "    halting_probability += p * still_running\n",
    "\n",
    "    # Compute remainders for the inputs which halted at this step\n",
    "    remainders += new_halted * (1 - halting_probability)\n",
    "\n",
    "    # Add the remainders to those inputs which halted at this step\n",
    "    halting_probability += new_halted * remainders\n",
    "\n",
    "    # Increment n_updates for all inputs which are still running\n",
    "    n_updates += still_running + new_halted\n",
    "\n",
    "    # Compute the weight to be applied to the new state and output\n",
    "    # 0 when the input has already halted\n",
    "    # p when the input hasn't halted yet\n",
    "    # the remainders when it halted this step\n",
    "    update_weights = p * still_running + new_halted * remainders\n",
    "\n",
    "    transformed_state = state\n",
    "\n",
    "    # TODO: change 3 to take hyperparameter\n",
    "    for i in range(3):\n",
    "        with tf.variable_scope(\"rec_layer_%d\" % i):\n",
    "            transformed_state = encoder_layer(state, i)\n",
    "\n",
    "    # update running part in the weighted state and keep the rest\n",
    "    new_state = ((transformed_state * update_weights) + (previous_state *\n",
    "                                                         (1 - update_weights)))\n",
    "\n",
    "    step += 1\n",
    "    return (transformed_state, step, halting_probability, remainders,\n",
    "            n_updates, new_state)\n",
    "\n",
    "\n",
    "def should_continue(u0, u1, halting_probability, u2, n_updates, u3):\n",
    "    \"\"\"While loop stops when this predicate is FALSE.\n",
    "    \n",
    "    I.e. all (probability < 1-eps AND counter < N) are false.\n",
    "    \n",
    "    Args:\n",
    "        u0: Not used\n",
    "        u1: Not used\n",
    "        halting_probability: halting probability\n",
    "        u2: Not used\n",
    "        n_updates: ACT n_updates\n",
    "        u3: Not used\n",
    "        \n",
    "    Returns:\n",
    "        bool\n",
    "    \"\"\"\n",
    "\n",
    "    del u0, u1, u2, u3\n",
    "    return tf.reduce_any(\n",
    "        tf.logical_and(tf.less(halting_probability, threshold),\n",
    "                       tf.less(n_updates, act_max_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T06:18:20.311799Z",
     "start_time": "2019-07-13T06:18:20.308501Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# batch_size = 32\n",
    "# seq_len = 500\n",
    "# dims = 128\n",
    "# act_max_steps = 4\n",
    "# threshold = 0.5\n",
    "# state = tf.random.uniform([batch_size, seq_len, dims])\n",
    "# step = 0\n",
    "\n",
    "# # remainders has to have same dims as halting_probability\n",
    "# halting_probability = tf.zeros([batch_size, seq_len, 1])\n",
    "# remainders = tf.zeros([batch_size, seq_len, 1])\n",
    "# n_updates = tf.zeros([batch_size, seq_len, 1])\n",
    "# previous_state = tf.zeros([batch_size, seq_len, dims])\n",
    "\n",
    "# (_, _, _, remainder, n_updates, new_state) = tf.while_loop(\n",
    "#     should_continue,\n",
    "#     ut_function,\n",
    "#     (state, step, halting_probability, remainders, n_updates, previous_state),\n",
    "#     maximum_iterations=act_max_steps + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:17:40.304323Z",
     "start_time": "2019-07-14T03:17:40.276664Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    seq_length = 10\n",
    "    vocab_size = 10 + 1 + 1\n",
    "    vocab_str = [f\"{i}\" for i in range(10)]\n",
    "    vocab_str += ['X', 'S']\n",
    "\n",
    "    batch_size = 32  # 12000\n",
    "    d_model = 128  # 512\n",
    "    #     d_model = 512\n",
    "    heads = 2\n",
    "    keep_prob = 1.0\n",
    "    n_layers = 2  # 6\n",
    "    #     n_layers = 6\n",
    "    d_ff = 256  # 2048\n",
    "    #     d_ff = 2048\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    step = 0\n",
    "    halting_probability = tf.zeros([batch_size, seq_length, 1])\n",
    "    remainders = tf.zeros([batch_size, seq_length, 1])\n",
    "    n_updates = tf.zeros([batch_size, seq_length, 1])\n",
    "    previous_state = tf.zeros([batch_size, seq_length, d_model])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    positional_encodings = generate_positional_encodings(d_model)\n",
    "    inputs = tf.placeholder(dtype=tf.int32,\n",
    "                            shape=(batch_size, seq_length), name=\"input\")\n",
    "    outputs = tf.placeholder(dtype=tf.int32,\n",
    "                             shape=(batch_size, seq_length), name=\"output\")\n",
    "    expected = tf.placeholder(dtype=tf.int32,\n",
    "                              shape=(batch_size, seq_length), name=\"expected\")\n",
    "    inputs_mask = tf.placeholder(dtype=tf.float32,\n",
    "                                 shape=(1, 1, seq_length),\n",
    "                                 name=\"input_mask\")\n",
    "    output_mask = tf.placeholder(dtype=tf.float32,\n",
    "                                 shape=(1, seq_length, seq_length),\n",
    "                                 name=\"output_mask\")\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name=\"learning_rate\")\n",
    "    w_embed, input_embeddings, output_embeddings = get_embeddings(inputs, outputs, vocab_size,\n",
    "                                                                  d_model)\n",
    "    input_embeddings = prepare_embeddings(input_embeddings,\n",
    "                                          positional_encodings=positional_encodings,\n",
    "                                          keep_prob=keep_prob,\n",
    "                                          is_input=True)\n",
    "    output_embeddings = prepare_embeddings(output_embeddings,\n",
    "                                           positional_encodings=positional_encodings,\n",
    "                                           keep_prob=keep_prob,\n",
    "                                           is_input=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     encoding = _encoder(input_embeddings, mask=inputs_mask, n_layers=n_layers, heads=heads,\n",
    "#                        keep_prob=keep_prob, d_ff=d_ff)\n",
    "\n",
    "    def encoder_layer(x, layer_num):\n",
    "#         return _encoder(x, inputs_mask, n_layers, heads, keep_prob, d_ff)\n",
    "\n",
    "        return _encoder_layer(x, inputs_mask, layer_num, \n",
    "                   heads, keep_prob, d_ff)\n",
    "\n",
    "#     def transition_function(x):\n",
    "#         return x\n",
    "    \n",
    "    def ut_function2(state, step, halting_probability, remainders, n_updates,\n",
    "                previous_state):\n",
    "        \n",
    "\n",
    "        return ut_function(state, step, halting_probability, remainders, n_updates,\n",
    "                previous_state, encoder_layer)\n",
    "    \n",
    "    \n",
    "    (_, _, _, remainder, n_updates, encoding) = tf.while_loop(\n",
    "        should_continue,\n",
    "        ut_function2,\n",
    "        (input_embeddings, step, halting_probability, remainders, n_updates, previous_state),\n",
    "        maximum_iterations=act_max_steps + 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    decoding = decoder(encoding, output_embeddings,\n",
    "                       enc_mask=inputs_mask, mask=output_mask,\n",
    "                       n_layers=n_layers, heads=heads, keep_prob=keep_prob, d_ff=d_ff)\n",
    "    log_results = generator(decoding, vocab_size=vocab_size)\n",
    "    results = tf.exp(log_results)\n",
    "    loss = label_smoothing_loss(log_results, expected, vocab_size=vocab_size, smoothing=0.0)\n",
    "    adam = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-5)\n",
    "    params = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, params)\n",
    "    print(grads)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 5.)\n",
    "    \n",
    "    grads_and_vars = list(zip(grads, params))\n",
    "    train_op = adam.apply_gradients(grads_and_vars, name=\"apply_gradients\")\n",
    "\n",
    "    warm_up = 400\n",
    "    batch_in_mask = np.ones((1, 1, seq_length), dtype=float)\n",
    "    batch_out_mask = output_subsequent_mask(seq_length)\n",
    "    batch_out_mask = batch_out_mask.reshape(1, seq_length, seq_length)\n",
    "    def __print_seq(seq):\n",
    "        return ' '.join([vocab_str[i] for i in seq])\n",
    "\n",
    "#     return\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(10000):\n",
    "            lr = noam_learning_rate(i + 1, warm_up, d_model)\n",
    "            \n",
    "            batch_in, batch_out = generate_data(batch_size, seq_length, vocab_size)\n",
    "            _, batch_loss, batch_res = session.run([train_op, loss, results],\n",
    "                                                   feed_dict={\n",
    "                                                       learning_rate: lr,\n",
    "                                                       inputs: batch_in,\n",
    "                                                       outputs: batch_out[:, :-1],\n",
    "                                                       expected: batch_out[:, 1:],\n",
    "                                                       inputs_mask: batch_in_mask,\n",
    "                                                       output_mask: batch_out_mask\n",
    "                                                   })\n",
    "            if i % 100 == 0:\n",
    "                print(f\"step={i}\\tloss={batch_loss: .6f}\")\n",
    "                print(f\"inp=  {__print_seq(batch_in[0])}\")\n",
    "                print(f\"exp={__print_seq(batch_out[0])}\")\n",
    "                print(f\"res=  {__print_seq(np.argmax(batch_res[0], -1))}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T03:18:03.521609Z",
     "start_time": "2019-07-14T03:17:40.623581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.framework.ops.IndexedSlices object at 0x13215c160>, <tf.Tensor 'gradients/prepare_input/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/prepare_input/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/prepare_output/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/prepare_output/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/sigmoid_activation_for_pondering/dense/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(128, 1) dtype=float32>, <tf.Tensor 'gradients/while/sigmoid_activation_for_pondering/dense/BiasAdd/Enter_grad/b_acc_3:0' shape=(1,) dtype=float32>, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/query/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/query/BiasAdd/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/key/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/key/BiasAdd/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/value/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/value/BiasAdd/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/attention/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/multi_head/attention/BiasAdd/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/norm/mul/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/attention_2/norm/add_1/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/ff_2/feed_forward/hidden/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(128, 256) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/ff_2/feed_forward/hidden/BiasAdd/Enter_grad/b_acc_3:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/ff_2/feed_forward/out/Tensordot/transpose_1/Enter_grad/b_acc_3:0' shape=(256, 128) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/ff_2/feed_forward/out/BiasAdd/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/ff_2/norm/mul/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/while/rec_layer_2/ff_2/norm/add_1/Enter_grad/b_acc_3:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/query/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/query/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/key/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/key/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/value/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/value/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/attention/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/multi_head/attention/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_self_attention/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/query/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/query/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/key/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/key/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/value/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/value/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/attention/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/multi_head/attention/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_encoding_attention/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_ff/feed_forward/hidden/Tensordot/transpose_1_grad/transpose:0' shape=(128, 256) dtype=float32>, <tf.Tensor 'gradients/decoder/0_ff/feed_forward/hidden/BiasAdd_grad/BiasAddGrad:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_ff/feed_forward/out/Tensordot/transpose_1_grad/transpose:0' shape=(256, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/0_ff/feed_forward/out/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_ff/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/0_ff/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/query/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/query/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/key/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/key/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/value/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/value/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/attention/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/multi_head/attention/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_self_attention/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/query/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/query/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/key/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/key/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/value/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/value/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/attention/Tensordot/transpose_1_grad/transpose:0' shape=(128, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/multi_head/attention/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_encoding_attention/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_ff/feed_forward/hidden/Tensordot/transpose_1_grad/transpose:0' shape=(128, 256) dtype=float32>, <tf.Tensor 'gradients/decoder/1_ff/feed_forward/hidden/BiasAdd_grad/BiasAddGrad:0' shape=(256,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_ff/feed_forward/out/Tensordot/transpose_1_grad/transpose:0' shape=(256, 128) dtype=float32>, <tf.Tensor 'gradients/decoder/1_ff/feed_forward/out/BiasAdd_grad/BiasAddGrad:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_ff/norm/mul_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/decoder/1_ff/norm/add_1_grad/Reshape_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'gradients/generator/Tensordot/transpose_1_grad/transpose:0' shape=(128, 12) dtype=float32>, <tf.Tensor 'gradients/generator/BiasAdd_grad/BiasAddGrad:0' shape=(12,) dtype=float32>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0\tloss= 0.061199\n",
      "inp=  6 9 3 4 1 7 8 4 9 5\n",
      "exp=S 5 X X 8 7 1 4 3 9 6\n",
      "res=  X X X X X X X X X X\n",
      "step=100\tloss= 0.048420\n",
      "inp=  4 0 5 9 6 0 3 4 8 6\n",
      "exp=S X 8 X 3 X 6 9 5 0 4\n",
      "res=  X X X X X X X X X X\n",
      "step=200\tloss= 0.045080\n",
      "inp=  1 5 6 2 8 0 2 0 1 5\n",
      "exp=S X X X X 0 8 2 6 5 1\n",
      "res=  X X X 1 1 1 2 1 1 1\n",
      "step=300\tloss= 0.045168\n",
      "inp=  4 0 0 2 1 7 1 1 8 5\n",
      "exp=S 5 8 1 X 7 1 2 X 0 4\n",
      "res=  X X X X X X X X 0 4\n",
      "step=400\tloss= 0.042045\n",
      "inp=  1 7 6 1 3 3 4 8 3 4\n",
      "exp=S X 3 8 4 X 3 X 6 7 1\n",
      "res=  X X X X X 1 X 6 1 1\n",
      "step=500\tloss= 0.039930\n",
      "inp=  4 1 5 0 0 9 4 6 7 6\n",
      "exp=S X 7 6 X 9 X 0 5 1 4\n",
      "res=  X X X X X X 0 4 4 4\n",
      "step=600\tloss= 0.039152\n",
      "inp=  5 9 3 8 4 4 5 7 9 5\n",
      "exp=S 5 X 7 X X 4 8 3 9 5\n",
      "res=  X X X X X 4 9 9 9 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-f23c21c7cab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-8ebd18d7f57c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m                                                        \u001b[0mexpected\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                                                        \u001b[0minputs_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_in_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                                                        \u001b[0moutput_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_out_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                                                    })\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
